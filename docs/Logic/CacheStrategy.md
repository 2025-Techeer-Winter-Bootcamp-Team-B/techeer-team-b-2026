# ğŸ—ƒï¸ ìºì‹± ì „ëµ (Cache Strategy)

Redis ê¸°ë°˜ ìºì‹± ì „ëµê³¼ êµ¬í˜„ ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

---

## ìºì‹± ê°œìš”

### ìºì‹± ëª©í‘œ

1. **ì‘ë‹µ ì†ë„ í–¥ìƒ**: DB ì¿¼ë¦¬ ëŒ€ì‹  ìºì‹œì—ì„œ ì¦‰ì‹œ ì‘ë‹µ
2. **DB ë¶€í•˜ ê°ì†Œ**: ë°˜ë³µ ì¿¼ë¦¬ ì œê±°
3. **Cold Start ë°©ì§€**: ì„œë²„ ì‹œì‘ ì‹œ ì£¼ìš” ë°ì´í„° ì‚¬ì „ ìºì‹±

### ìºì‹± íë¦„

```
ìš”ì²­ ìˆ˜ì‹ 
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    ìºì‹œ í‚¤ ìƒì„±      â”‚
â”‚    (í•´ì‹œ ê¸°ë°˜)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
      ìºì‹œ ì¡°íšŒ
          â”‚
    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â”‚           â”‚
   HIT        MISS
    â”‚           â”‚
    â–¼           â–¼
ìºì‹œ ë°˜í™˜    DB ì¿¼ë¦¬
              â”‚
              â–¼
         ìºì‹œ ì €ì¥
              â”‚
              â–¼
          ì‘ë‹µ ë°˜í™˜
```

---

## ìºì‹œ í‚¤ ì„¤ê³„

### í•´ì‹œ ê¸°ë°˜ ìºì‹œ í‚¤

íŒŒë¼ë¯¸í„°ê°€ ë§ì•„ë„ ê³ ì • ê¸¸ì´ì˜ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```python
# app/utils/cache.py
import hashlib
import orjson

def generate_hash_key(*args, **kwargs) -> str:
    """í•´ì‹œ ê¸°ë°˜ ìºì‹œ í‚¤ ìƒì„±"""
    key_data = orjson.dumps({
        "args": args,
        "kwargs": sorted(kwargs.items())
    })
    hash_value = hashlib.md5(key_data).hexdigest()
    return f"realestate:{hash_value}"
```

**ì˜ˆì‹œ:**
```python
key = generate_hash_key(
    "statistics", "rvol",
    region_type="ì „êµ­",
    transaction_type="sale",
    period_months=3
)
# â†’ "realestate:a1b2c3d4e5f6..."
```

### ê³„ì¸µì  ìºì‹œ í‚¤

```python
def build_cache_key(*parts: str) -> str:
    """ê³„ì¸µì  ìºì‹œ í‚¤ ìƒì„±"""
    return "realestate:" + ":".join(str(p) for p in parts)
```

**ì˜ˆì‹œ:**
```python
key = build_cache_key("apartment", "12345", "detail")
# â†’ "realestate:apartment:12345:detail"
```

---

## TTL ì „ëµ

### ë°ì´í„° ìœ í˜•ë³„ TTL

| ë°ì´í„° ìœ í˜• | TTL | ì´ìœ  |
|------------|-----|------|
| í™ˆ í™”ë©´ (ì˜ˆì—´) | 12ì‹œê°„ | ë¶€ë™ì‚° ë°ì´í„° ì—…ë°ì´íŠ¸ ë¹ˆë„ ë‚®ìŒ |
| í†µê³„ ë°ì´í„° | 6ì‹œê°„ | ë³µì¡í•œ ì§‘ê³„, ìì£¼ ë³€í•˜ì§€ ì•ŠìŒ |
| ì•„íŒŒíŠ¸ ìƒì„¸ | 10ë¶„ | ìì£¼ ë³€ê²½ë˜ì§€ ì•ŠìŒ |
| ê²€ìƒ‰ ê²°ê³¼ | 5ë¶„ | ë¹ ë¥¸ ì‘ë‹µ í•„ìš” |
| ì„¸ì…˜ ë°ì´í„° | 1ì‹œê°„ | ë³´ì•ˆìƒ ì§§ê²Œ ìœ ì§€ |

### TTL ìƒìˆ˜ ì •ì˜

```python
# app/core/config.py
class CacheSettings:
    HOME_PRELOAD_TTL = 43200    # 12ì‹œê°„
    STATISTICS_TTL = 21600      # 6ì‹œê°„
    APARTMENT_DETAIL_TTL = 600  # 10ë¶„
    SEARCH_RESULT_TTL = 300     # 5ë¶„
    SESSION_TTL = 3600          # 1ì‹œê°„
```

---

## ìºì‹œ êµ¬í˜„

### ê¸°ë³¸ ìºì‹œ í•¨ìˆ˜

```python
# app/utils/cache.py
from redis.asyncio import Redis
import orjson
from typing import Optional, Any

redis_client: Optional[Redis] = None

async def get_redis_client() -> Redis:
    """Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    global redis_client
    if redis_client is None:
        redis_client = Redis.from_url(settings.REDIS_URL)
    return redis_client

async def get_from_cache(key: str) -> Optional[Any]:
    """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
    client = await get_redis_client()
    data = await client.get(key)
    if data:
        return orjson.loads(data)
    return None

async def set_to_cache(key: str, data: Any, ttl: int = 3600) -> bool:
    """ìºì‹œì— ë°ì´í„° ì €ì¥"""
    client = await get_redis_client()
    try:
        await client.setex(key, ttl, orjson.dumps(data))
        return True
    except Exception as e:
        logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

async def delete_from_cache(key: str) -> bool:
    """ìºì‹œì—ì„œ ë°ì´í„° ì‚­ì œ"""
    client = await get_redis_client()
    await client.delete(key)
    return True

async def delete_cache_pattern(pattern: str) -> int:
    """íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìºì‹œ ì¼ê´„ ì‚­ì œ"""
    client = await get_redis_client()
    cursor = 0
    deleted = 0
    
    while True:
        cursor, keys = await client.scan(cursor, match=pattern, count=100)
        if keys:
            await client.delete(*keys)
            deleted += len(keys)
        if cursor == 0:
            break
    
    return deleted
```

---

## ìºì‹œ ì˜ˆì—´ (Cache Warmup)

### ì„œë²„ ì‹œì‘ ì‹œ ìºì‹œ ì˜ˆì—´

```python
# app/main.py
@app.on_event("startup")
async def startup_event():
    # Redis ì—°ê²° ì´ˆê¸°í™”
    await get_redis_client()
    
    # ë°±ê·¸ë¼ìš´ë“œë¡œ ìºì‹œ ì˜ˆì—´ (ì„œë²„ ì‹œì‘ ë¸”ë¡œí‚¹ ì—†ìŒ)
    asyncio.create_task(preload_all_caches())

async def preload_all_caches():
    """ëª¨ë“  ìºì‹œ ì˜ˆì—´"""
    try:
        # 1. í™ˆ í™”ë©´ ìºì‹œ
        await preload_home_cache()
        
        # 2. í†µê³„ ìºì‹œ
        await preload_statistics_cache()
        
        logger.info("âœ… ìºì‹œ ì˜ˆì—´ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ì˜ˆì—´ ì‹¤íŒ¨: {e}")
```

### í™ˆ í™”ë©´ ìºì‹œ ì˜ˆì—´

```python
# app/services/warmup.py
async def preload_home_cache():
    """í™ˆ í™”ë©´ ìºì‹œ ì˜ˆì—´"""
    PRELOAD_TTL = 43200  # 12ì‹œê°„
    
    cache_tasks = [
        # ëŒ€ì‹œë³´ë“œ ìš”ì•½
        ("dashboard/summary", {"transaction_type": "sale", "months": 6}),
        ("dashboard/summary", {"transaction_type": "jeonse", "months": 6}),
        # ë­í‚¹
        ("dashboard/rankings", {"transaction_type": "sale"}),
        ("dashboard/rankings", {"transaction_type": "jeonse"}),
    ]
    
    async with AsyncSessionLocal() as db:
        for endpoint, params in cache_tasks:
            try:
                # ë°ì´í„° ì¡°íšŒ
                data = await fetch_dashboard_data(db, endpoint, params)
                
                # ìºì‹œ ì €ì¥
                cache_key = generate_hash_key(endpoint, **params)
                await set_to_cache(cache_key, data, ttl=PRELOAD_TTL)
                
                logger.info(f"âœ… {endpoint} ìºì‹± ì™„ë£Œ")
            except Exception as e:
                logger.error(f"âŒ {endpoint} ìºì‹± ì‹¤íŒ¨: {e}")
```

### í†µê³„ ìºì‹œ ì˜ˆì—´

```python
async def preload_statistics_cache():
    """í†µê³„ ìºì‹œ ì˜ˆì—´"""
    region_types = ["ì „êµ­", "ìˆ˜ë„ê¶Œ", "ì§€ë°©5ëŒ€ê´‘ì—­ì‹œ"]
    transaction_types = ["sale", "jeonse"]
    max_years_options = [1, 3, 5, 10]
    
    async with AsyncSessionLocal() as db:
        for region_type in region_types:
            for transaction_type in transaction_types:
                for max_years in max_years_options:
                    # RVOL ìºì‹±
                    await cache_rvol(db, region_type, transaction_type)
                    
                    # ê±°ë˜ëŸ‰ ì¶”ì´ ìºì‹±
                    await cache_transaction_volume(
                        db, region_type, transaction_type, max_years
                    )
```

---

## ìºì‹œ ë¬´íš¨í™”

### ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œ ìºì‹œ ë¬´íš¨í™”

```python
# app/services/cache_invalidation.py
async def invalidate_apartment_cache(apt_id: int):
    """ì•„íŒŒíŠ¸ ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”"""
    patterns = [
        f"realestate:apartment:{apt_id}:*",
        f"realestate:*:apt:{apt_id}:*",
    ]
    
    for pattern in patterns:
        deleted = await delete_cache_pattern(pattern)
        logger.debug(f"ìºì‹œ ì‚­ì œ: {pattern}, {deleted}ê±´")

async def invalidate_statistics_cache(
    region_id: Optional[int] = None,
    transaction_type: Optional[str] = None
):
    """í†µê³„ ìºì‹œ ë¬´íš¨í™”"""
    patterns = []
    
    if region_id:
        patterns.append(f"realestate:statistics:*:region:{region_id}:*")
    
    if transaction_type:
        patterns.append(f"realestate:statistics:*:type:{transaction_type}:*")
    
    for pattern in patterns:
        await delete_cache_pattern(pattern)
```

### ìŠ¤ì¼€ì¤„ëŸ¬ ê¸°ë°˜ ê°±ì‹ 

```python
# app/services/statistics_cache_scheduler.py
from apscheduler.schedulers.asyncio import AsyncIOScheduler

scheduler = AsyncIOScheduler()

async def start_statistics_scheduler():
    """í†µê³„ ìºì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
    
    # ë§¤ì¼ ìƒˆë²½ 2ì‹œì— í†µê³„ ìºì‹œ ê°±ì‹ 
    scheduler.add_job(
        precompute_all_statistics,
        trigger="cron",
        hour=2,
        minute=0
    )
    
    scheduler.start()
    logger.info("í†µê³„ ìºì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨")
```

---

## ì„±ëŠ¥ ì¸¡ì •

### ìºì‹œ íˆíŠ¸ìœ¨ ëª¨ë‹ˆí„°ë§

```python
import prometheus_client as prom

cache_hits = prom.Counter("cache_hits_total", "Total cache hits")
cache_misses = prom.Counter("cache_misses_total", "Total cache misses")

async def get_from_cache_with_metrics(key: str) -> Optional[Any]:
    data = await get_from_cache(key)
    if data:
        cache_hits.inc()
    else:
        cache_misses.inc()
    return data
```

### ì„±ëŠ¥ ê°œì„  íš¨ê³¼

| ì§€í‘œ | ìºì‹œ ì—†ìŒ | ìºì‹œ ì ìš© |
|------|----------|----------|
| í†µê³„ API ì‘ë‹µ | 3-5ì´ˆ | **50-100ms** |
| ì²« ìš”ì²­ (Cold) | 5ì´ˆ+ | **100ms (ì˜ˆì—´ í›„)** |
| DB ì¿¼ë¦¬ ìˆ˜ | 100íšŒ/ë¶„ | **10íšŒ/ë¶„** |
| ìºì‹œ íˆíŠ¸ìœ¨ | 0% | **80%+** |
